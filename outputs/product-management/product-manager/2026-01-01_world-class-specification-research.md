# –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ: –ú–∏—Ä–æ–≤—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –Ω–∞–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤—ã—Ö —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π

**–î–∞—Ç–∞**: 2026-01-01
**–ê–≥–µ–Ω—Ç**: product-manager

## –†–µ–∑—é–º–µ

–ü—Ä–æ–≤–µ–¥–µ–Ω–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫ –≤ –≤–µ–¥—É—â–∏—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∫–æ–º–ø–∞–Ω–∏—è—Ö: Amazon, Google, Stripe, Apple, Netflix. –í—ã—è–≤–ª–µ–Ω–æ **47 –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —É–ª—É—á—à–µ–Ω–∏–π** –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ spec-template.md –∏ **12 –Ω–æ–≤—ã—Ö –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤–æ—Ä–æ—Ç**. –¢–µ–∫—É—â–∏–π —à–∞–±–ª–æ–Ω –ø–æ–∫—Ä—ã–≤–∞–µ—Ç ~65% world-class —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤.

---

## 1. –°–ü–ï–¶–ò–§–ò–ö–ê–¶–ò–û–ù–ù–´–ï –§–†–ï–ô–ú–í–û–†–ö–ò –¢–û–ü–û–í–´–• –ö–û–ú–ü–ê–ù–ò–ô

### 1.1 Amazon: PRFAQ + 6-Pager

**–ò—Å—Ç–æ—á–Ω–∏–∫**: [Working Backwards PRFAQ](https://productstrategy.co/working-backwards-the-amazon-prfaq-for-product-innovation/), [Product School PRFAQ Guide](https://productschool.com/blog/product-fundamentals/prfaq)

**–°—É—Ç—å**: Customer-centric –¥–æ–∫—É–º–µ–Ω—Ç, –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã–π —Å —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –¥–Ω—è –∑–∞–ø—É—Å–∫–∞. –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∑–∏—Å: "Start with the customer and work backwards."

**–°—Ç—Ä—É–∫—Ç—É—Ä–∞ PRFAQ**:

#### Press Release (1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞)
- **Headline**: Customer-focused –∑–∞–≥–æ–ª–æ–≤–æ–∫
- **Subheadline**: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
- **Date**: –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ –∑–∞–ø—É—Å–∫–∞
- **Intro paragraph**: –†–µ—à–µ–Ω–∏–µ, —Ü–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è, –≤—ã–≥–æ–¥—ã
- **Problem** (2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è): –ü—Ä–æ–±–ª–µ–º–∞ –∫–ª–∏–µ–Ω—Ç–∞
- **Solution** (2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è): –ö–∞–∫ –ø—Ä–æ–¥—É–∫—Ç —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É
- **Quote from Leadership**: –ü–æ—á–µ–º—É –º—ã –≤ —ç—Ç–æ –≤–µ—Ä–∏–º
- **Customer Quote**: –í–æ–æ–±—Ä–∞–∂–∞–µ–º—ã–π –æ—Ç–∑—ã–≤ —Ä–∞–Ω–Ω–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π

#### FAQ (External)
–í–æ–ø—Ä–æ—Å—ã –æ—Ç –∫–ª–∏–µ–Ω—Ç–æ–≤ –∏ –ø—Ä–µ—Å—Å—ã:
- How does it work?
- What is the warranty?
- How do I install it?
- What is the return policy?

#### FAQ (Internal)
–í–æ–ø—Ä–æ—Å—ã –æ—Ç —Å—Ç–µ–π–∫—Ö–æ–ª–¥–µ—Ä–æ–≤ (finance, marketing, ops, HR, legal):
- What are the estimated engineering hours?
- Are there external vendor costs?
- What is the total cost to deliver and maintain?
- What are the technical/financial/legal challenges?

**6-Pager Process**:
- –ù–∞—Ä—Ä–∞—Ç–∏–≤–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –¥–æ 6 —Å—Ç—Ä–∞–Ω–∏—Ü (–Ω–µ bullets, –∞ –ø—Ä–æ–∑–∞)
- –ü–µ—Ä–≤—ã–µ 20 –º–∏–Ω—É—Ç –º–∏—Ç–∏–Ω–≥–∞ ‚Äî –º–æ–ª—á–∞–ª–∏–≤–æ–µ —á—Ç–µ–Ω–∏–µ (no pre-read)
- –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –≤—Å–µ –ø—Ä–æ—á–∏—Ç–∞–ª–∏ –æ–¥–Ω—É –∏ —Ç—É –∂–µ –≤–µ—Ä—Å–∏—é

**–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã**:
1. **Customer obsession**: –ù–∞—á–∞—Ç—å —Å –∫–ª–∏–µ–Ω—Ç–∞
2. **Clarity of thought**: –ù–∞—Ä—Ä–∞—Ç–∏–≤ –≤–º–µ—Å—Ç–æ bullets –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç –¥—É–º–∞—Ç—å –≥–ª—É–±–∂–µ
3. **No PowerPoint**: –ü—Ä–æ–∑–∞ = –ø—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–∏–∫–∏
4. **Silent reading**: –£—Å—Ç—Ä–∞–Ω—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—É—é –∞—Å–∏–º–º–µ—Ç—Ä–∏—é

**–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å –∫ Spec-Kit**:
- ‚úÖ –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Å–µ–∫—Ü–∏—é "PRFAQ Summary" –¥–ª—è product-facing features
- ‚úÖ FAQ format –¥–ª—è –ø—Ä–µ–¥–≤–æ—Å—Ö–∏—â–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ —Å—Ç–µ–π–∫—Ö–æ–ª–¥–µ—Ä–æ–≤
- ‚ùå Full 6-pager —Å–ª–∏—à–∫–æ–º —Ç—è–∂–µ–ª–æ–≤–µ—Å–µ–Ω –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —Ñ–∏—á

---

### 1.2 Google: Design Docs

**–ò—Å—Ç–æ—á–Ω–∏–∫**: [Design Docs at Google](https://www.industrialempathy.com/posts/design-docs-at-google/), [Google Design Doc Template](https://docs.google.com/document/d/1uMHzRsEDZb_p9xfFGerCVhr-0mAi-d-OFY4jJi0dYk4/edit)

**–°—É—Ç—å**: High-level implementation strategy —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ **trade-offs** –ø—Ä–∏ –ø—Ä–∏–Ω—è—Ç–∏–∏ —Ä–µ—à–µ–Ω–∏–π.

**–°—Ç—Ä—É–∫—Ç—É—Ä–∞**:

1. **Context/Background**
   - Objective background facts
   - Landscape –æ–±–∑–æ—Ä
   - –°—Å—ã–ª–∫–∏ –Ω–∞ –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é

2. **Goals and Non-Goals**
   - Bullet points (3-5 items)
   - **Non-goals –∫—Ä–∏—Ç–∏—á–Ω—ã**: –ß—Ç–æ —è–≤–Ω–æ –≤–Ω–µ —Å–∫–æ—É–ø–∞

3. **Alternative Designs/Tradeoffs** ‚≠ê **–ö–õ–Æ–ß–ï–í–ê–Ø –°–ï–ö–¶–ò–Ø**
   - –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–ª–∏—Å—å
   - Trade-offs –∫–∞–∂–¥–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞
   - **–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –≤—ã–±–æ—Ä–∞** —Ç–µ–∫—É—â–µ–≥–æ –¥–∏–∑–∞–π–Ω–∞
   - "Do nothing" –∫–∞–∫ baseline –≤–∞—Ä–∏–∞–Ω—Ç

4. **Cross-Cutting Concerns**
   - Security
   - Privacy
   - Observability
   - Performance
   - Accessibility

5. **Solution Summary**
   - System diagram
   - Code examples
   - Wire frames (–¥–ª—è UI)

**–ö–æ–≥–¥–∞ –ø–∏—Å–∞—Ç—å Design Doc**:
- ‚úÖ –†–µ—à–µ–Ω–∏–µ –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ–µ (complexity + ambiguity)
- ‚ùå –ï—Å–ª–∏ doc –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç—Å—è –≤ implementation manual –±–µ–∑ trade-offs

**Benefits**:
- Early identification of issues (cheap to fix)
- Organizational consensus
- Senior review opportunity
- Knowledge transfer artifact

**–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å –∫ Spec-Kit**:
- ‚úÖ **–ö–†–ò–¢–ò–ß–ù–û**: –î–æ–±–∞–≤–∏—Ç—å —Å–µ–∫—Ü–∏—é "Alternative Designs & Trade-offs" –≤ plan.md
- ‚úÖ "Goals and Non-Goals" —É–∂–µ –µ—Å—Ç—å, –Ω–æ –Ω–∞–¥–æ —É—Å–∏–ª–∏—Ç—å Non-Goals
- ‚úÖ Cross-cutting concerns —á–∞—Å—Ç–∏—á–Ω–æ –ø–æ–∫—Ä—ã—Ç—ã, –Ω—É–∂–Ω–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å

---

### 1.3 Stripe: RFC (Request for Comments)

**–ò—Å—Ç–æ—á–Ω–∏–∫**: [Pragmatic Engineer: RFCs and Design Docs](https://newsletter.pragmaticengineer.com/p/software-engineering-rfc-and-design), [LeadDev: Team Guide to RFCs](https://leaddev.com/software-quality/thorough-team-guide-rfcs)

**–°—É—Ç—å**: –ü–∏—Å—å–º–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å —Ü–µ–ª—å—é –ø–æ–ª—É—á–∏—Ç—å feedback –∏ —Å—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –¥–∏—Å–∫—É—Å—Å–∏—é –æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏—è—Ö.

**–§–∏–ª–æ—Å–æ—Ñ–∏—è RFC**:
- Build software faster by clarifying assumptions early
- Circulate plans before implementation
- Similar to TDD: test your architecture before building

**–ö–æ–º–ø–∞–Ω–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ RFCs**:
- Stripe
- Airbnb (specs for Product + Engineering)
- Uber
- Spotify

**–¢–∏–ø–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ RFC**:
- Problem statement
- Proposed solution
- Alternative solutions considered
- Risks and unknowns
- Success criteria
- Open questions

**Benefits**:
- Decision documentation (why, not just what)
- Reduces rework
- Better maintainability
- Shared understanding

**–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å –∫ Spec-Kit**:
- ‚úÖ spec.md —É–∂–µ –∏–º–µ–µ—Ç —á–µ—Ä—Ç—ã RFC (problem ‚Üí solution)
- ‚úÖ –î–æ–±–∞–≤–∏—Ç—å "Open Questions" —Å–µ–∫—Ü–∏—é
- ‚úÖ –£—Å–∏–ª–∏—Ç—å "Risks and Unknowns" coverage

---

### 1.4 Apple: Human Interface Guidelines (HIG) Integration

**–ò—Å—Ç–æ—á–Ω–∏–∫**: [Apple HIG Official](https://developer.apple.com/design/human-interface-guidelines/), [iOS Design Guidelines 2025](https://tapptitude.com/blog/i-os-app-design-guidelines-for-2025)

**–°—É—Ç—å**: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–∏–∑–∞–π–Ω-—Å–∏—Å—Ç–µ–º—ã –≤ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—é —Å –ø–µ—Ä–≤–æ–≥–æ –¥–Ω—è.

**2025 Update: Liquid Glass Design Language**
- Translucency, depth, fluid responsiveness
- Unified look across iOS 26, iPadOS 26, macOS 26
- Significant redesign since 2013

**Core HIG Principles**:
1. **Clarity**: Clean, precise, uncluttered
2. **Consistency**: Standard UI elements, familiar patterns
3. **Deference**: UI doesn't distract from content
4. **Depth**: Layers, shadows, motion –¥–ª—è visual hierarchy

**Integration Approach**:
- SF Symbols library (6,900+ icons)
- Design tokens via Apple Design Resources
- Accessibility baked in (VoiceOver, keyboard nav)
- Platform-specific adaptations (iPhone, iPad, Mac, Watch)

**–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å –∫ Spec-Kit**:
- ‚úÖ –°–µ–∫—Ü–∏—è "Design System" —É–∂–µ –µ—Å—Ç—å, –Ω–æ —Å–ª–∞–±–æ –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞
- ‚úÖ –î–æ–±–∞–≤–∏—Ç—å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ —Å—Å—ã–ª–∞—Ç—å—Å—è –Ω–∞ platform HIG
- ‚úÖ Icon System, Animation Library ‚Äî —Ö–æ—Ä–æ—à–µ–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ
- ‚úÖ Accessibility –Ω–µ –∫–∞–∫ checkbox, –∞ –∫–∞–∫ design principle

---

### 1.5 Netflix: Experimentation-First Approach

**–ò—Å—Ç–æ—á–Ω–∏–∫**: [Netflix A/B Testing Platform](http://techblog.netflix.com/2016/04/its-all-about-testing-netflix.html), [Return-Aware Experimentation](https://netflixtechblog.medium.com/return-aware-experimentation-3dd93c94b67a), [Netflix Personalization](https://medium.com/@productbrief/netflixs-personalization-powerhouse-how-a-b-testing-at-scale-built-a-300b-streaming-giant-f26804e0d92c)

**–°—É—Ç—å**: –í—Å–µ product –∏–¥–µ–∏ –ø—Ä–æ—Ö–æ–¥—è—Ç —á–µ—Ä–µ–∑ scientific method ‚Äî data, not opinion.

**Culture**:
- "Any new idea can be developed and tested"
- Seniority –Ω–µ –≤–ª–∏—è–µ—Ç –Ω–∞ decision
- Embedded in infrastructure

**2025 Innovations**:
- **Return-Aware Framework**: Optimize for long-run returns
- **Heterogeneous Treatment Effects (HTE)**: Causal impact analysis
- Tradeoff: more low-powered tests vs fewer high-powered tests
- Relax p-value thresholds for faster iteration

**Experimentation Architecture**:
- Modular design: SQL, Python, R contributions
- Production + local workflows use same codebase
- Causal inference methodology acceleration

**Key Metrics**:
- Search success rate
- Time to play
- User satisfaction

**–ü—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å –∫ Spec-Kit**:
- ‚úÖ **–ö–†–ò–¢–ò–ß–ù–û**: –î–æ–±–∞–≤–∏—Ç—å "Experiment Design" —Å–µ–∫—Ü–∏—é
- ‚úÖ Hypothesis ‚Üí Metrics ‚Üí Decision criteria
- ‚úÖ A/B test plan –¥–ª—è measurable features
- ‚ö†Ô∏è Requires product analytics infrastructure

---

## 2. –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –≠–õ–ï–ú–ï–ù–¢–´ WORLD-CLASS –°–ü–ï–¶–ò–§–ò–ö–ê–¶–ò–ô

### 2.1 User Story Depth & Acceptance Criteria Quality

**Best Practices** (—Å–æ–±—Ä–∞–Ω–æ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤):

#### Structure
- **Given-When-Then** format (Cucumber/Gherkin style)
- Unique IDs –¥–ª—è traceability (AS-1A, AS-1B)
- –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è (P1a, P1b, P2a)
- "Requires Test" column –¥–ª—è enforcement

‚úÖ **Spec-Kit —É–∂–µ –∏–º–µ–µ—Ç**: ID, Given-When-Then, Requires Test
‚ö†Ô∏è **–ù—É–∂–Ω–æ —É—Å–∏–ª–∏—Ç—å**: –ë–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏–µ acceptance criteria

#### Depth Indicators (world-class)
1. **Boundary conditions** —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω—ã
2. **Error states** –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã
3. **Performance expectations** quantified (e.g., "<2 seconds")
4. **Accessibility behavior** specified (e.g., "Focus moves to error field")
5. **Data validation rules** explicit (e.g., "Email format: RFC 5322")

**–ü—Ä–∏–º–µ—Ä —É–ª—É—á—à–µ–Ω–∏—è**:

‚ùå **Weak**:
```
| AS-1A | User fills form | User submits | Form is saved | YES |
```

‚úÖ **Strong**:
```
| AS-1A | User fills all required fields (name 2-50 chars, email RFC 5322, phone E.164) | User clicks Submit (<100ms response) | Form saves to DB within 500ms, success message appears, focus moves to next action, screen reader announces success | YES |
```

---

### 2.2 Edge Case & Error Handling Coverage

**–ò—Å—Ç–æ—á–Ω–∏–∫**: [Edge Cases in Software Testing](https://muuktest.com/blog/edge-cases-in-software-testing), [Error Handling Best Practices](https://www.bomberbot.com/testing/a-beginners-guide-to-testing-error-handling-edge-cases/)

#### –ú–µ—Ç–æ–¥—ã –≤—ã—è–≤–ª–µ–Ω–∏—è Edge Cases

1. **Boundary Value Analysis**
   - Min, min+1, max-1, max –∑–Ω–∞—á–µ–Ω–∏—è
   - Empty, single item, max capacity
   - –ü—Ä–∏–º–µ—Ä: Username 6-12 chars ‚Üí test 5, 6, 12, 13

2. **Input Testing**
   - Negative values, null, special characters
   - Unicode edge cases (emoji, RTL text)
   - Injection attempts (XSS, SQL)

3. **Monkey/Fuzz Testing**
   - Random inputs
   - Unexpected sequences
   - Real-world interruptions (phone call, battery drain)

4. **State-based Edge Cases**
   - Concurrent operations
   - Network failures mid-transaction
   - Session expiration during action

‚úÖ **Spec-Kit —É–∂–µ –∏–º–µ–µ—Ç**: Edge Cases —Å–µ–∫—Ü–∏—è —Å CRITICAL flag
‚ö†Ô∏è **–ù—É–∂–Ω–æ —É—Å–∏–ª–∏—Ç—å**: Structured edge case discovery framework

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞**:

```markdown
### Edge Cases

| ID | Category | Condition | Expected Behavior | Critical | Test ID |
|----|----------|-----------|-------------------|----------|---------|
| EC-001 | Boundary | Input length = 0 chars | Error: "Required field" | | T020 |
| EC-002 | Security | SQL injection attempt | Input sanitized, logged | CRITICAL | T021 |
| EC-003 | Concurrency | Two users edit same record | Last write wins + conflict notification | | T022 |
| EC-004 | Network | API timeout after 30s | Retry 3x, then user-friendly error | | T023 |
| EC-005 | State | User session expires mid-form | Auto-save draft, redirect to login | | T024 |
```

---

### 2.3 Security & Privacy Considerations

**–ò—Å—Ç–æ—á–Ω–∏–∫**: [WCAG 2.2 ISO Standard](https://www.w3.org/press-releases/2025/wcag22-iso-pas/), [Data Privacy & Accessibility](https://termly.io/resources/articles/data-privacy-web-accessibility-compliance-link/)

#### Security Requirements Framework

**STRIDE Model** (Microsoft):
- **S**poofing: Identity verification
- **T**ampering: Data integrity
- **R**epudiation: Audit logging
- **I**nformation Disclosure: Data encryption
- **D**enial of Service: Rate limiting
- **E**levation of Privilege: Authorization

**Spec-Kit Coverage**:
‚úÖ Partial: Edge cases with CRITICAL flag –¥–ª—è security
‚ùå Missing: Structured STRIDE analysis
‚ùå Missing: Privacy impact assessment (GDPR/CCPA)

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è —Å–µ–∫—Ü–∏—è**:

```markdown
### Security & Privacy Requirements

#### Threat Model
| ID | Threat (STRIDE) | Attack Vector | Mitigation | FR Reference |
|----|-----------------|---------------|------------|--------------|
| SEC-001 | Spoofing | Session hijacking | HttpOnly cookies, CSRF tokens | FR-005 |
| SEC-002 | Information Disclosure | API response leaks PII | Field-level filtering | FR-012 |

#### Privacy Requirements (GDPR/CCPA)
| ID | Requirement | Implementation | Verification |
|----|-------------|----------------|--------------|
| PII-001 | Right to erasure | DELETE /user/:id endpoint | Manual test |
| PII-002 | Data minimization | Collect only email, not phone | Code review |

#### Compliance Checklist
- [ ] OWASP Top 10 reviewed
- [ ] PII handling documented
- [ ] Encryption at rest and in transit
- [ ] Audit logging implemented
```

---

### 2.4 Performance Requirements & SLOs

**–ò—Å—Ç–æ—á–Ω–∏–∫**: [OpenTelemetry Metrics](https://opentelemetry.io/docs/concepts/signals/metrics/), [Telemetry-Driven DevOps](https://devops.com/delivery-executive-ai-analytics/)

**Service Level Objectives (SLO) Framework**:

1. **Availability**: Uptime percentage
2. **Latency**: Response time percentiles (p50, p95, p99)
3. **Throughput**: Requests per second
4. **Error Rate**: % of failed requests

‚úÖ **Spec-Kit —É–∂–µ –∏–º–µ–µ—Ç**: Performance goals –≤ plan.md
‚ö†Ô∏è **–ù—É–∂–Ω–æ —É—Å–∏–ª–∏—Ç—å**: SLO specification –≤ spec.md

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞**:

```markdown
### Performance Requirements

| ID | Metric | Target (SLO) | Measurement Method | Degradation Behavior |
|----|--------|--------------|-------------------|----------------------|
| PERF-001 | API Response (p95) | <200ms | OpenTelemetry traces | Queue requests if >500ms |
| PERF-002 | Page Load (p50) | <1.5s | WebPageTest | Show skeleton UI |
| PERF-003 | Database Query | <50ms | Query logs | Add indexes if >100ms |
| PERF-004 | Throughput | 1000 req/s | Load testing | Auto-scale at 80% capacity |

#### Load Test Scenarios
| Scenario | Users | Duration | Success Criteria |
|----------|-------|----------|------------------|
| Normal load | 100 concurrent | 10 min | All SLOs met |
| Peak load | 500 concurrent | 5 min | p95 <300ms |
| Stress test | 1000 concurrent | 2 min | No errors, graceful degradation |
```

---

### 2.5 Accessibility Requirements (WCAG)

**–ò—Å—Ç–æ—á–Ω–∏–∫**: [WCAG 2.2 ISO 40500:2025](https://www.w3.org/press-releases/2025/wcag22-iso-pas/), [WCAG Complete Guide](https://www.allaccessible.org/blog/wcag-22-complete-guide-2025)

**2025 Compliance Landscape**:
- WCAG 2.2 = ISO/IEC 40500:2025 (Oct 2025)
- EU Accessibility Act (EAA) ‚Äî mandatory —Å 28 June 2025
- ADA + Section 508 + EAA ‚Äî –≤—Å–µ —Å—Å—ã–ª–∞—é—Ç—Å—è –Ω–∞ WCAG 2.2 AA

**WCAG 2.2: 9 –Ω–æ–≤—ã—Ö success criteria**:
1. Focus Not Obscured (Minimum) ‚Äî AA
2. Focus Not Obscured (Enhanced) ‚Äî AAA
3. Focus Appearance ‚Äî AAA
4. Dragging Movements ‚Äî AA (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ –¥–ª—è drag-and-drop)
5. Target Size (Minimum) ‚Äî AA (24x24px min touch target)
6. Consistent Help ‚Äî A
7. Redundant Entry ‚Äî A (don't ask twice)
8. Accessible Authentication (Minimum) ‚Äî AA
9. Accessible Authentication (Enhanced) ‚Äî AAA

‚úÖ **Spec-Kit —É–∂–µ –∏–º–µ–µ—Ç**: Accessibility as Empowerment (UXQ)
‚ö†Ô∏è **–ù—É–∂–Ω–æ —É—Å–∏–ª–∏—Ç—å**: WCAG 2.2 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ success criteria

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞**:

```markdown
### Accessibility Requirements (WCAG 2.2 Level AA)

| WCAG Criterion | Level | Requirement | Implementation | Verification |
|----------------|-------|-------------|----------------|--------------|
| 1.4.3 Contrast (Minimum) | AA | Text contrast ‚â•4.5:1 | Design tokens enforce | Automated test |
| 2.1.1 Keyboard | A | All functionality via keyboard | Focus management | Manual test |
| 2.4.7 Focus Visible | AA | Focus indicator always visible | 2px outline | Manual test |
| 2.5.5 Target Size (Enhanced) | AAA | Touch targets ‚â•44x44px | Component library | Automated test |
| 3.2.6 Consistent Help | A | Help access in same location | Header component | Manual test |
| 3.3.7 Redundant Entry | A | Autofill previously entered data | Form state management | Manual test |
| 3.3.8 Accessible Authentication | AA | No cognitive function test for auth | Magic link option | Manual test |

#### Assistive Technology Testing
- [ ] Screen reader (NVDA/JAWS/VoiceOver)
- [ ] Keyboard-only navigation
- [ ] Voice control (Dragon/Voice Control)
- [ ] High contrast mode
- [ ] 200% zoom
```

---

### 2.6 Internationalization (i18n) Considerations

**–ò—Å—Ç–æ—á–Ω–∏–∫**: [W3C i18n Best Practices](https://www.w3.org/TR/international-specs/), [i18n Testing Guide](https://aqua-cloud.io/internationalization-testing/)

**Core Requirements**:

1. **Character Encoding**
   - UTF-8 everywhere (prevent text corruption)
   - Normalization (NFC/NFD)

2. **Text Expansion**
   - German +20-25% vs English
   - Flexible layouts (relative widths, not fixed px)

3. **RTL Support**
   - Right-to-left languages (Arabic, Hebrew)
   - Bidirectional text (mixing RTL + LTR)

4. **Locale-Aware Formatting**
   - Dates, times, numbers, currencies
   - ICU standard (International Components for Unicode)
   - Pluralization rules (vary by language)

5. **Cultural Adaptation**
   - Color meanings (red = danger in West, lucky in China)
   - Icons and symbols
   - Legal requirements (GDPR in EU, different in China)

‚úÖ **Spec-Kit currently**: No i18n coverage
‚ùå **Missing**: Internationalization requirements section

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞**:

```markdown
### Internationalization Requirements

| ID | Requirement | Implementation | Priority |
|----|-------------|----------------|----------|
| I18N-001 | UTF-8 encoding everywhere | Database, API, frontend | P1 |
| I18N-002 | Text expansion buffer (+30%) | CSS flex/grid, no fixed widths | P1 |
| I18N-003 | RTL layout support | CSS logical properties | P2 |
| I18N-004 | Locale-aware formatting | i18next + ICU MessageFormat | P1 |
| I18N-005 | Extractable strings | No hardcoded text in code | P1 |
| I18N-006 | Pseudo-localization testing | CI pipeline check | P2 |

#### Supported Locales (Initial)
- en-US (English, United States)
- es-ES (Spanish, Spain)
- zh-CN (Chinese, Simplified)
- ar-SA (Arabic, Saudi Arabia) ‚Äî RTL

#### Translation Workflow
1. Extract strings ‚Üí en.json
2. Send to translation service
3. Import translated files
4. Pseudo-localization test
5. Native speaker review
```

---

### 2.7 Analytics & Success Measurement

**–ò—Å—Ç–æ—á–Ω–∏–∫**: [OpenTelemetry Standard](https://opentelemetry.io/), [Telemetry Best Practices](https://www.splunk.com/en_us/blog/learn/what-is-telemetry.html)

**Instrumentation Strategy**:

1. **Product Measurement Planning**
   - Define success metrics before building
   - Map data readiness
   - Prioritize events

2. **OpenTelemetry (Industry Standard)**
   - Metrics, logs, traces
   - 50% IT orgs use OTel (2025 report)
   - Platform-agnostic

3. **Event Taxonomy**
   - User actions (clicks, form submissions)
   - System events (errors, performance)
   - Business events (conversions, revenue)

‚úÖ **Spec-Kit —É–∂–µ –∏–º–µ–µ—Ç**: Success Criteria
‚ö†Ô∏è **–ù—É–∂–Ω–æ —É—Å–∏–ª–∏—Ç—å**: Telemetry specification

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞**:

```markdown
### Analytics & Instrumentation

#### Success Metrics
| Metric | Type | Target | Measurement | Dashboard |
|--------|------|--------|-------------|-----------|
| Task completion rate | Product | 90% | Event: task_completed | Mixpanel |
| Time to first value | Product | <60s | Event: first_action_completed | Amplitude |
| Error rate | System | <1% | OpenTelemetry traces | Datadog |
| API latency (p95) | System | <200ms | OpenTelemetry metrics | Grafana |

#### Event Tracking Plan
| Event Name | Trigger | Properties | Priority |
|------------|---------|------------|----------|
| user_signup | Form submission success | {method, source} | P1 |
| feature_used | User clicks feature button | {feature_id, timestamp} | P1 |
| error_occurred | Exception thrown | {error_type, stack_trace} | P1 |

#### Privacy & Compliance
- [ ] No PII in event properties (hash user_id)
- [ ] GDPR consent for analytics
- [ ] Data retention policy (90 days)
```

---

## 3. –ö–ê–ß–ï–°–¢–í–ï–ù–ù–´–ï –í–û–†–û–¢–ê (QUALITY GATES)

**–ò—Å—Ç–æ—á–Ω–∏–∫**: [Quality Gates Checklist](https://medium.com/@dneprokos/quality-gates-the-watchers-of-software-quality-af19b177e5d1), [FDA Software Validation](https://www.fda.gov/media/73141/download), [2025 Validation Readiness](https://www.sqasolution.com/validation-readiness-checklist-2025/)

### 3.1 –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ Gates –≤ Spec-Kit

‚úÖ **Constitution Check** (—É–∂–µ –µ—Å—Ç—å):
- Complexity tracking
- Pattern violations

‚úÖ **/speckit.analyze** (—É–∂–µ –µ—Å—Ç—å):
- Pass A: Acceptance scenario coverage
- Pass B: Edge case coverage
- Pass C: Cross-artifact consistency
- Pass W: Test-to-spec traceability

### 3.2 –ù–µ–¥–æ—Å—Ç–∞—é—â–∏–µ World-Class Gates

**Gate 1: Requirements Completeness**
- [ ] All functional requirements have IDs
- [ ] All FRs map to ‚â•1 acceptance scenario
- [ ] All "NEEDS CLARIFICATION" resolved
- [ ] No vague requirements (subjective terms)

**Gate 2: Testability**
- [ ] All "Requires Test = YES" have test IDs in tasks.md
- [ ] All CRITICAL edge cases have tests
- [ ] Test framework specified
- [ ] Mock/fixture requirements documented

**Gate 3: Security Review**
- [ ] STRIDE threat model completed
- [ ] All CRITICAL edge cases address security
- [ ] PII handling documented
- [ ] OWASP Top 10 considered

**Gate 4: Accessibility Compliance**
- [ ] WCAG level specified (A/AA/AAA)
- [ ] Screen reader compatibility verified
- [ ] Keyboard navigation specified
- [ ] Minimum touch targets defined (44x44px)

**Gate 5: Performance Validation**
- [ ] SLOs defined (latency, throughput, error rate)
- [ ] Load test scenarios specified
- [ ] Degradation behavior documented
- [ ] Monitoring/alerting plan exists

**Gate 6: Internationalization Readiness**
- [ ] Supported locales documented
- [ ] Text expansion buffer planned
- [ ] RTL support specified (if needed)
- [ ] Cultural considerations reviewed

**Gate 7: Analytics Instrumentation**
- [ ] Success metrics defined
- [ ] Event tracking plan exists
- [ ] Privacy compliance verified
- [ ] Dashboard/reporting plan exists

**Gate 8: Design System Consistency** (–¥–ª—è UI features)
- [ ] Component library specified
- [ ] Design tokens referenced
- [ ] Accessibility level aligned with HIG
- [ ] Responsive breakpoints defined

**Gate 9: API Contract Validation** (–¥–ª—è API features)
- [ ] OpenAPI spec generated
- [ ] Breaking changes documented
- [ ] Versioning strategy defined
- [ ] Deprecation timeline specified

**Gate 10: Documentation Completeness**
- [ ] User-facing docs planned
- [ ] API reference exists
- [ ] Migration guide (for breaking changes)
- [ ] Troubleshooting guide

**Gate 11: Stakeholder Approval**
- [ ] Product owner sign-off
- [ ] Engineering feasibility confirmed
- [ ] Security team approval (for CRITICAL features)
- [ ] Legal/compliance review (for PII/payments)

**Gate 12: Traceability Validation**
- [ ] All FR ‚Üí AS mappings valid
- [ ] All AS ‚Üí Test mappings valid
- [ ] All dependencies documented
- [ ] System spec impact defined

### 3.3 –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ Spec-Kit

**–ù–æ–≤–∞—è –∫–æ–º–∞–Ω–¥–∞**: `/speckit.validate`

```bash
specify validate [--gate=all|requirements|security|perf|...] [--strict]
```

**Output**:
```
‚úÖ Gate 1: Requirements Completeness ‚Äî PASS
‚úÖ Gate 2: Testability ‚Äî PASS
‚ö†Ô∏è Gate 3: Security Review ‚Äî WARNING (2 issues)
   - STRIDE model incomplete
   - PII handling not documented
‚ùå Gate 4: Accessibility Compliance ‚Äî FAIL
   - WCAG level not specified
   - Touch target sizes not defined
...

Score: 8/12 gates passed (66%)
```

---

## 4. –ê–ù–¢–ò-–ü–ê–¢–¢–ï–†–ù–´ –°–ü–ï–¶–ò–§–ò–ö–ê–¶–ò–ô

**–ò—Å—Ç–æ—á–Ω–∏–∫**: [Specification Drift Prevention](https://www.xugj520.cn/en/archives/ai-code-documentation-sync-tool.html), [Software Anti-patterns](https://medium.com/@srinathperera/a-deeper-look-at-software-architecture-anti-patterns-9ace30f59354)

### 4.1 Implementation Drift (–î—Ä–µ–π—Ñ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏)

**–ü—Ä–æ–±–ª–µ–º–∞**: Code –æ—Ç—Ö–æ–¥–∏—Ç –æ—Ç spec —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º.

**–ü—Ä–∏—á–∏–Ω—ã**:
1. Spec –Ω–µ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –ø–æ—Å–ª–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π
2. –ù–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
3. Spec –∏ code –∂–∏–≤—É—Ç –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö

**–†–µ—à–µ–Ω–∏–µ**:
- **Semcheck** (AI-powered tool): LLM –ø—Ä–æ–≤–µ—Ä—è–µ—Ç code vs spec
- **CI/CD integration**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–∏ PR
- **Living documentation**: Specs –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è –≤–º–µ—Å—Ç–µ —Å code

**Spec-Kit Approach**:
‚úÖ `/speckit.merge` ‚Äî –æ–±–Ω–æ–≤–ª—è–µ—Ç system specs –ø–æ—Å–ª–µ merge
‚úÖ Feature specs = historical, system specs = current
‚ö†Ô∏è –ù—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å: CI check –¥–ª—è spec-code alignment

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**:

```yaml
# .github/workflows/spec-validation.yml
name: Spec Validation
on: [pull_request]
jobs:
  validate:
    steps:
      - run: specify analyze --strict
      - run: specify validate --all-gates
      - run: semcheck verify  # external tool
```

---

### 4.2 Vague Requirements

**–ê–Ω—Ç–∏-–ø–∞—Ç—Ç–µ—Ä–Ω**: "System MUST be fast", "UI MUST be user-friendly"

**–ü—Ä–æ–±–ª–µ–º–∞**: –°—É–±—ä–µ–∫—Ç–∏–≤–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –±–µ–∑ measurable criteria

**–†–µ—à–µ–Ω–∏–µ**: SMART requirements
- **S**pecific: –¢–æ—á–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
- **M**easurable: –ö–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞
- **A**chievable: –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ
- **R**elevant: –°–≤—è–∑–∞–Ω–æ —Å –±–∏–∑–Ω–µ—Å-—Ü–µ–ª—å—é
- **T**ime-bound: Deadline –∏–ª–∏ milestone

**–ü—Ä–∏–º–µ—Ä—ã —É–ª—É—á—à–µ–Ω–∏—è**:

‚ùå "System MUST be fast"
‚úÖ "API MUST respond in <200ms (p95) for read requests"

‚ùå "UI MUST be user-friendly"
‚úÖ "90% of users MUST complete signup in <2 minutes on first attempt (measured via analytics)"

---

### 4.3 Unstable Interface (–ù–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å)

**–ê–Ω—Ç–∏-–ø–∞—Ç—Ç–µ—Ä–Ω**: API –º–µ–Ω—è–µ—Ç—Å—è —Ç–∞–∫ —á–∞—Å—Ç–æ, —á—Ç–æ –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—è —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –±–µ—Å–ø–æ–ª–µ–∑–Ω–æ–π

**–ü—Ä–æ–±–ª–µ–º–∞**: Clients –ª–æ–º–∞—é—Ç—Å—è –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∞–ø–¥–µ–π—Ç–µ

**–†–µ—à–µ–Ω–∏–µ**:
1. **Semantic Versioning**: Major.Minor.Patch
2. **Deprecation Policy**: 2+ versions warning period
3. **API Contracts**: OpenAPI spec as source of truth
4. **Breaking Change Documentation**: Migration guide

**Spec-Kit Coverage**:
‚úÖ API Contracts –≤ plan.md
‚úÖ Breaking Changes –≤ spec.md
‚ö†Ô∏è –ù—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å: Deprecation timeline enforcement

---

### 4.4 Linguistic Anti-patterns

**–ò—Å—Ç–æ—á–Ω–∏–∫**: [IEEE: Linguistic Anti-patterns](https://ieeexplore.ieee.org/document/6498467/)

**–ü—Ä–æ–±–ª–µ–º–∞**: –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–µ–∂–¥—É:
- Method signature
- Documentation
- Actual behavior

**–ü—Ä–∏–º–µ—Ä**:
```python
def getUserById(id):
    """Returns user object."""
    return user_dict  # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict, –Ω–µ object!
```

**–†–µ—à–µ–Ω–∏–µ**:
- Type hints –≤ –∫–æ–¥–µ
- Contract testing
- API schema validation (OpenAPI, GraphQL schema)

**Spec-Kit**: –£–∂–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç OpenAPI contracts ‚úÖ

---

### 4.5 Big Ball of Mud (–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã)

**–ê–Ω—Ç–∏-–ø–∞—Ç—Ç–µ—Ä–Ω**: System –±–µ–∑ –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞–µ–º–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

**–ü—Ä–æ–±–ª–µ–º–∞**: –ö–∞–∂–¥–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ç—Ä–µ–±—É–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏—è –≤—Å–µ–π —Å–∏—Å—Ç–µ–º—ã

**–†–µ—à–µ–Ω–∏–µ**:
- **Architectural Decision Records (ADR)**: –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–π decisions
- **Domain-Driven Design**: –Ø–≤–Ω—ã–µ boundaries
- **Modular architecture**: Loose coupling

**Spec-Kit Coverage**:
‚úÖ Constitution –¥–ª—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤
‚ö†Ô∏è –ù—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å: ADR template

---

## 5. EMERGING BEST PRACTICES (2025)

### 5.1 AI-Augmented Specification Writing

**–ò—Å—Ç–æ—á–Ω–∏–∫**: [Spec-Driven Development 2025](https://www.softwareseni.com/spec-driven-development-in-2025-the-complete-guide-to-using-ai-to-write-production-code/), [GitHub Spec-Kit Guide](https://github.blog/ai-and-ml/generative-ai/spec-driven-development-with-ai-get-started-with-a-new-open-source-toolkit/)

**–¢—Ä–µ–Ω–¥**: Specification ‚Üí AI Generation ‚Üí Validation

**Workflow**:
1. Requirements ‚Üí Detailed Spec
2. AI consumes spec ‚Üí generates code
3. Validation (spec-to-test automation)

**–ü–ª–∞—Ç—Ñ–æ—Ä–º—ã (15+ launched 2024-2025)**:
- **GitHub Spec-Kit** (yours!)
- AWS Kiro (enterprise, 3-phase: Specify ‚Üí Plan ‚Üí Execute)
- OpenSpec (lightweight, locks intent before implementation)

**Key Principle**: **Spec as source of truth**
- CI/CD auto-regenerates code when spec changes
- Tests generated from specs
- Docs generated from specs

**Real-World Results**:
- **Google**: 80% code auto-generated, 50% migration time reduction
- **Airbnb**: 3,500 test files in 6 weeks (vs 1.5 years manual)

**Spec-Kit Positioning**: ‚úÖ Already aligned with this trend!

---

### 5.2 Living Documentation

**–ö–æ–Ω—Ü–µ–ø—Ü–∏—è**: Documentation –∂–∏–≤–µ—Ç —Ä—è–¥–æ–º —Å –∫–æ–¥–æ–º –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏

**–ü–æ–¥—Ö–æ–¥—ã**:
1. **Docs as Code**: Markdown –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏
2. **Auto-generation**: From code comments, type hints, schemas
3. **Continuous Deployment**: Docs deploy –Ω–∞ –∫–∞–∂–¥—ã–π commit

**Tools**:
- **Storybook** (UI components)
- **Swagger UI** (API docs)
- **Docusaurus** (static site generator)

**Spec-Kit Coverage**:
‚úÖ Specs –≤ git —Ä—è–¥–æ–º —Å –∫–æ–¥–æ–º
‚úÖ System specs –∫–∞–∫ living docs
‚ö†Ô∏è –ù—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å: Auto-deploy docs workflow

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**:

```yaml
# Auto-generate docs from specs
specify docs generate --output=docs/
specify docs deploy --to=github-pages
```

---

### 5.3 Spec-to-Test Automation

**–¢—Ä–µ–Ω–¥**: –¢–µ—Å—Ç—ã –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç—Å—è –∏–∑ specs –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏

**–ü–æ–¥—Ö–æ–¥—ã**:

1. **Contract Testing**
   - OpenAPI spec ‚Üí Contract tests (Pact, Spring Cloud Contract)
   - GraphQL schema ‚Üí Apollo Federation tests

2. **BDD (Behavior-Driven Development)**
   - Given-When-Then ‚Üí Cucumber tests
   - Acceptance scenarios ‚Üí Automated E2E tests

3. **Property-Based Testing**
   - Spec defines properties ‚Üí Hypothesis/QuickCheck generates tests

**Spec-Kit Coverage**:
‚úÖ Acceptance scenarios —Å Given-When-Then
‚ö†Ô∏è –ù—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å: Auto-generate test stubs from AS-xxx

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**:

```bash
# New command
specify test generate --from=spec.md --framework=pytest

# Output: tests/test_feature.py with stubs
def test_AS_1A():
    """AS-1A: Given user fills form, When submits, Then saved"""
    # TODO: Implement test
    pass
```

---

### 5.4 Design Systems as Code

**–¢—Ä–µ–Ω–¥**: Design tokens ‚Üí Code generation

**Workflow**:
1. Design –≤ Figma —Å tokens plugin
2. Export tokens.json
3. Generate CSS variables, TypeScript types, native code

**Tools**:
- **Style Dictionary** (Amazon)
- **Theo** (Salesforce)
- **Figma Tokens** plugin

**Spec-Kit Coverage**:
‚úÖ Design System —Å–µ–∫—Ü–∏—è –≤ spec.md
‚ö†Ô∏è –ù—É–∂–Ω–æ —É—Å–∏–ª–∏—Ç—å: Token import/export workflow

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**:

```bash
# Import design tokens from Figma
specify design import --from=figma://file_key --output=design-tokens.json

# Generate code from tokens
specify design generate --tokens=design-tokens.json --platform=web
```

---

### 5.5 Observability-Driven Development

**–ö–æ–Ω—Ü–µ–ø—Ü–∏—è**: Build telemetry into spec from day 1

**Principle**: "Born reportable" ‚Äî observability is foundation, not afterthought

**Framework**:
1. Define success metrics in spec
2. Design event taxonomy
3. Implement instrumentation alongside features
4. Validate metrics in CI

**OpenTelemetry Coverage**:
- Metrics (stable in 2025)
- Traces (mature)
- Logs (recently stabilized)

**Spec-Kit Coverage**:
‚ö†Ô∏è Analytics —Å–µ–∫—Ü–∏—è —Å–ª–∞–±–∞—è, –Ω—É–∂–Ω–æ —É—Å–∏–ª–∏—Ç—å

---

## 6. –ö–û–ù–ö–†–ï–¢–ù–´–ï –£–õ–£–ß–®–ï–ù–ò–Ø –î–õ–Ø SPEC-KIT

### 6.1 –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ spec-template.md

#### 1. Amazon PRFAQ Summary (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–µ–∫—Ü–∏—è –¥–ª—è product features)

```markdown
## PRFAQ Summary *(for customer-facing features)*

### Press Release
**Headline**: [Customer-focused headline]
**Problem**: [2-4 sentences: customer pain]
**Solution**: [2-4 sentences: how we solve it]
**Customer Quote**: [Imagined early adopter testimonial]

### FAQ (External)
1. **Q**: How does it work?
   **A**: [Brief explanation]
2. **Q**: What are the benefits?
   **A**: [Key value props]

### FAQ (Internal)
1. **Q**: What are the estimated costs?
   **A**: [Engineering hours, infra, vendor costs]
2. **Q**: What are the risks?
   **A**: [Technical, financial, legal risks]
```

---

#### 2. Alternative Designs & Trade-offs (Google Design Doc style)

```markdown
## Design Decisions *(mandatory for plan.md)*

### Alternative Solutions Considered

| Alternative | Pros | Cons | Why Not Selected |
|-------------|------|------|------------------|
| Do Nothing | No cost | Problem persists | Unacceptable user pain |
| Solution A | [pros] | [cons] | [trade-off rationale] |
| **Selected: Solution B** | [pros] | [cons] | **Best balance of X vs Y** |

### Key Trade-offs
1. **[Trade-off 1]**: Chose X over Y because [rationale]
2. **[Trade-off 2]**: Sacrificed X to gain Y because [business priority]

### Assumptions & Risks
- **Assumption**: [What we assume to be true]
- **Risk**: [What could go wrong] ‚Üí Mitigation: [how we handle it]
```

---

#### 3. Security & Privacy Requirements (STRIDE + GDPR)

```markdown
## Security & Privacy *(mandatory for features handling data)*

### Threat Model (STRIDE)
| ID | Threat Type | Attack Vector | Mitigation | Priority |
|----|-------------|---------------|------------|----------|
| SEC-001 | Spoofing | Session hijacking | HttpOnly cookies, CSRF tokens | P1 |
| SEC-002 | Tampering | SQL injection | Parameterized queries, ORM | P1 |
| SEC-003 | Repudiation | User denies action | Audit logging, signatures | P2 |
| SEC-004 | Info Disclosure | API leaks PII | Field filtering, encryption | P1 |
| SEC-005 | DoS | API flooding | Rate limiting, WAF | P2 |
| SEC-006 | Elevation | Unauthorized access | RBAC, principle of least privilege | P1 |

### Privacy Requirements (GDPR/CCPA)
| ID | Requirement | Implementation | Verification |
|----|-------------|----------------|--------------|
| PII-001 | Right to erasure | DELETE /user/:id, cascade delete | Manual test |
| PII-002 | Data minimization | Collect only necessary fields | Code review |
| PII-003 | Consent management | Explicit opt-in for analytics | UI flow |
| PII-004 | Data portability | Export user data as JSON | API endpoint |

### Compliance Checklist
- [ ] OWASP Top 10 threats addressed
- [ ] PII handling documented
- [ ] Encryption: TLS 1.3 in transit, AES-256 at rest
- [ ] Audit logging: WHO did WHAT WHEN
- [ ] Secrets management: No credentials in code
```

---

#### 4. Performance Requirements & SLOs

```markdown
## Performance Requirements *(mandatory for production features)*

### Service Level Objectives (SLOs)
| ID | Metric | Target | Measurement | Alerting Threshold |
|----|--------|--------|-------------|-------------------|
| PERF-001 | API Latency (p95) | <200ms | OpenTelemetry | >300ms |
| PERF-002 | API Latency (p99) | <500ms | OpenTelemetry | >800ms |
| PERF-003 | Error Rate | <0.1% | Error tracking | >0.5% |
| PERF-004 | Throughput | 1000 req/s | Load balancer metrics | <800 req/s |
| PERF-005 | Availability | 99.9% (43min/month downtime) | Uptime monitoring | <99.5% |

### Load Test Scenarios
| Scenario | Users | Duration | Success Criteria |
|----------|-------|----------|------------------|
| Baseline | 10 concurrent | 5 min | All SLOs met, no errors |
| Normal load | 100 concurrent | 10 min | All SLOs met |
| Peak load | 500 concurrent | 5 min | p95 <300ms, error rate <1% |
| Stress test | 1000 concurrent | 2 min | Graceful degradation, no crashes |
| Soak test | 50 concurrent | 2 hours | No memory leaks, stable latency |

### Degradation Strategy
- **80% capacity**: Trigger auto-scaling
- **95% capacity**: Queue requests, show "High load" message
- **100% capacity**: Return 503 Service Unavailable, exponential backoff
```

---

#### 5. Accessibility Requirements (WCAG 2.2)

```markdown
## Accessibility Requirements *(mandatory for UI features)*

### Compliance Target
**WCAG Level**: [A / AA / AAA]
**Legal Requirements**: [ADA / Section 508 / EAA / None]

### WCAG 2.2 Success Criteria (Level AA)

#### Perceivable
| ID | Criterion | Requirement | Implementation | Test Method |
|----|-----------|-------------|----------------|-------------|
| 1.1.1 | Non-text Content | All images have alt text | img alt="..." | Automated (axe) |
| 1.4.3 | Contrast (Minimum) | Text contrast ‚â•4.5:1 | Design tokens enforce | Automated (axe) |
| 1.4.11 | Non-text Contrast | UI components contrast ‚â•3:1 | CSS variables | Automated |

#### Operable
| ID | Criterion | Requirement | Implementation | Test Method |
|----|-----------|-------------|----------------|-------------|
| 2.1.1 | Keyboard | All functionality via keyboard | Focus management, no mouse-only | Manual |
| 2.4.7 | Focus Visible | Focus indicator always visible | :focus-visible 2px outline | Manual |
| 2.5.5 | Target Size | Touch targets ‚â•44x44px | Component library min-width/height | Automated |
| 2.5.8 | Target Size (Minimum) | Targets ‚â•24x24px (WCAG 2.2 new) | Same as above | Automated |

#### Understandable
| ID | Criterion | Requirement | Implementation | Test Method |
|----|-----------|-------------|----------------|-------------|
| 3.2.6 | Consistent Help | Help in same location (WCAG 2.2) | Header component | Manual |
| 3.3.7 | Redundant Entry | Autofill repeated data (WCAG 2.2) | Form state | Manual |
| 3.3.8 | Accessible Authentication | No cognitive test (WCAG 2.2) | Magic link option | Manual |

#### Robust
| ID | Criterion | Requirement | Implementation | Test Method |
|----|-----------|-------------|----------------|-------------|
| 4.1.2 | Name, Role, Value | ARIA labels for custom components | aria-label, role | Automated (axe) |

### Assistive Technology Testing Plan
- [ ] **NVDA** (Windows screen reader) ‚Äî Primary user: Jane (blind)
- [ ] **JAWS** (Windows screen reader) ‚Äî Secondary validation
- [ ] **VoiceOver** (macOS/iOS screen reader) ‚Äî Mobile testing
- [ ] **Keyboard-only navigation** ‚Äî All flows completable
- [ ] **Dragon NaturallySpeaking** (voice control) ‚Äî Voice command support
- [ ] **High contrast mode** ‚Äî Windows High Contrast, Dark mode
- [ ] **200% zoom** ‚Äî Content reflows, no horizontal scroll
```

---

#### 6. Internationalization Requirements

```markdown
## Internationalization *(mandatory for global products)*

### Supported Locales
| Locale | Language | Region | Launch Priority | RTL |
|--------|----------|--------|----------------|-----|
| en-US | English | United States | P1 (MVP) | No |
| es-ES | Spanish | Spain | P2 | No |
| ar-SA | Arabic | Saudi Arabia | P3 | Yes |

### Technical Requirements
| ID | Requirement | Implementation | Verification |
|----|-------------|----------------|--------------|
| I18N-001 | UTF-8 encoding | Database, API, frontend all UTF-8 | Config audit |
| I18N-002 | Text expansion (+30% buffer) | CSS flex/grid, no fixed px widths | Visual test |
| I18N-003 | RTL layout support | CSS logical properties (inline-start) | ar-SA locale test |
| I18N-004 | Locale-aware formatting | i18next + ICU MessageFormat | Unit tests |
| I18N-005 | Extractable strings | No hardcoded text in JSX/templates | Linter rule |
| I18N-006 | Pluralization | ICU plural rules (one, few, many) | Unit tests |
| I18N-007 | Date/time formatting | Intl.DateTimeFormat API | Locale tests |
| I18N-008 | Number/currency formatting | Intl.NumberFormat API | Locale tests |

### Translation Workflow
1. **Extract**: `npm run i18n:extract` ‚Üí en.json
2. **Translate**: Upload to translation service (Crowdin, Lokalise)
3. **Import**: Download translated files ‚Üí locales/
4. **Pseudo-localization**: CI test with [[ ∆Ä≈ï√•√ßƒ∑ƒô≈ß≈ü ]]
5. **Review**: Native speaker validation

### Cultural Considerations
| Locale | Adaptation | Reason |
|--------|------------|--------|
| zh-CN | Different success color (red = lucky) | Cultural meaning |
| de-DE | Formal "Sie" vs informal "du" | Language formality |
| ar-SA | Friday-Saturday weekend | Regional calendar |
```

---

#### 7. Analytics & Instrumentation

```markdown
## Analytics & Instrumentation *(mandatory for measurable features)*

### Success Metrics (Product)
| Metric ID | Metric | Type | Target | Measurement | Dashboard |
|-----------|--------|------|--------|-------------|-----------|
| PM-001 | Task completion rate | Product | 90% | Event: task_completed / task_started | Mixpanel |
| PM-002 | Time to first value | Product | <60s | Event: first_value_achieved | Amplitude |
| PM-003 | User retention (D7) | Product | 60% | Cohort analysis | Custom SQL |
| PM-004 | Feature adoption | Product | 40% in 30 days | Event: feature_first_use | Mixpanel |

### System Metrics (OpenTelemetry)
| Metric ID | Metric | Type | Target | Measurement | Dashboard |
|-----------|--------|------|--------|-------------|-----------|
| SYS-001 | API latency (p95) | System | <200ms | OTEL traces | Grafana |
| SYS-002 | Error rate | System | <0.1% | OTEL metrics | Datadog |
| SYS-003 | Database query time (p95) | System | <50ms | OTEL spans | Grafana |

### Event Tracking Plan
| Event Name | Trigger | Properties | Priority | Privacy |
|------------|---------|------------|----------|---------|
| user_signup | Signup form submit success | {method: email/oauth, source: organic/paid} | P1 | Hash user_id |
| feature_used | Feature button clicked | {feature_id, timestamp, context} | P1 | No PII |
| error_occurred | Exception thrown | {error_type, message (sanitized), stack_trace} | P1 | Sanitize sensitive data |
| task_completed | User completes main task | {task_id, duration, steps_count} | P1 | Hash user_id |

### Instrumentation Requirements
- **OpenTelemetry SDK**: Auto-instrumentation for HTTP, DB, cache
- **Custom Spans**: Wrap business-critical operations
- **Context Propagation**: Trace ID across services
- **Sampling**: 100% for errors, 10% for normal requests

### Privacy & Compliance
- [ ] No PII in event properties (hash user_id, email)
- [ ] GDPR consent for analytics cookies
- [ ] Data retention: 90 days for events, 1 year for aggregates
- [ ] Right to erasure: DELETE /analytics/:user_id endpoint
```

---

#### 8. Experiment Design (Netflix-style)

```markdown
## Experiment Design *(for measurable hypothesis-driven features)*

### Hypothesis
**We believe that** [change description]
**Will result in** [expected outcome]
**We will know we are right when** [measurable success criteria]

**Example**:
- We believe that adding social proof (X users signed up today)
- Will result in 15% increase in signup conversion
- We will know we are right when signup_rate (treatment) > signup_rate (control) with p<0.05

### Experiment Configuration
| Parameter | Value |
|-----------|-------|
| Experiment Name | signup_social_proof_v1 |
| Hypothesis | Social proof increases conversions |
| Start Date | 2026-02-01 |
| Duration | 14 days (2 full weeks) |
| Sample Size | 10,000 users (80% power, 5% significance) |
| Traffic Split | 50% control, 50% treatment |
| Primary Metric | signup_conversion_rate |
| Secondary Metrics | time_on_page, bounce_rate |

### Success Criteria
| Metric | Control Baseline | Treatment Target | Statistical Significance |
|--------|------------------|------------------|--------------------------|
| Signup rate | 12% | 13.8% (+15%) | p < 0.05 |
| Time on page | 45s | No degradation (<-10%) | p < 0.05 |
| Bounce rate | 60% | No increase (<+5%) | p < 0.05 |

### Guardrail Metrics (Must Not Degrade)
- Page load time (p95) < 1.5s
- Error rate < 0.1%
- User satisfaction (survey) > 4.0/5.0

### Decision Framework
- **Ship**: Primary metric +15% AND all guardrails pass
- **Iterate**: Primary metric +5-15% OR one guardrail fails ‚Üí redesign
- **Kill**: Primary metric <+5% OR multiple guardrails fail ‚Üí abandon

### Rollback Plan
- **Trigger**: Error rate >1% OR load time >3s
- **Action**: Feature flag off within 5 minutes
- **Notification**: Alert on-call engineer + product manager
```

---

### 6.2 –ù–æ–≤—ã–µ —Å–µ–∫—Ü–∏–∏ –¥–ª—è plan-template.md

#### 1. Trade-offs & Alternatives (Google Design Doc)

```markdown
## Design Decisions & Trade-offs *(mandatory)*

### Problem Analysis
**Core Problem**: [What are we solving?]
**Why Now**: [Why is this problem urgent?]
**Do Nothing Baseline**: [What happens if we don't build this?]

### Solutions Considered

#### Alternative 1: [Name]
**Approach**: [Brief description]
**Pros**:
- [Advantage 1]
- [Advantage 2]

**Cons**:
- [Limitation 1]
- [Limitation 2]

**Why Not Selected**: [Trade-off rationale]

#### Alternative 2: [Name]
[Same structure]

#### ‚úÖ Selected Solution: [Name]
**Approach**: [Brief description]
**Pros**: [Why this is better]
**Cons**: [Acknowledged limitations]
**Trade-offs Accepted**:
1. Chose X over Y because [business priority / technical constraint]
2. Sacrificed X to gain Y because [user value / maintainability]

### Key Assumptions
| ID | Assumption | Risk if Wrong | Validation Method |
|----|------------|---------------|-------------------|
| ASM-001 | Users will accept 2s load time | Churn increases | A/B test |
| ASM-002 | API rate limit 1000/min sufficient | Service degradation | Load testing |

### Open Questions
| Question | Impact | Owner | Deadline |
|----------|--------|-------|----------|
| [Unresolved question] | [High/Med/Low] | [Name] | [Date] |
```

---

#### 2. API Verification Phase (Context7 Integration)

```markdown
## Phase 0.5: API Verification *(auto-executed by /speckit.plan)*

### Dependency Verification Status

#### PKG-001: [package-name] ^X.Y.Z
- ‚úÖ Version exists in npm registry
- ‚úÖ Documentation fetched from Context7
- ‚úÖ Key methods verified:
  - `method1(params)` ‚Üí returns Type (docs: [link])
  - `method2()` ‚Üí deprecated since vX.Y, use method3() (docs: [link])
- ‚ö†Ô∏è Breaking changes in version X.Y+1: [summary]

#### API-001: Stripe API 2024-12-18
- ‚úÖ API version valid
- ‚úÖ Endpoints verified:
  - POST /v1/customers ‚Üí Creates customer (docs: [link])
  - GET /v1/charges/:id ‚Üí Retrieves charge (docs: [link])
- ‚ö†Ô∏è Rate limit: 100 req/s (consider if peak load >80 req/s)

### Hallucination Prevention
**Deprecated APIs to Avoid**:
| Package | Deprecated Method | Replacement | Since Version |
|---------|------------------|-------------|---------------|
| react | componentWillMount | useEffect hook | 16.3.0 |
| stripe | tokens.create | paymentMethods.create | 2019-12-03 |

**Common Mistakes**:
- ‚ùå Using `stripe.charges.create()` for new integrations
- ‚úÖ Use `stripe.paymentIntents.create()` instead (SCA compliant)
```

---

### 6.3 –ù–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã –¥–ª—è Spec-Kit

#### 1. `/speckit.validate` ‚Äî Quality Gate Validation

```bash
specify validate [OPTIONS]

OPTIONS:
  --gate=<name>     Run specific gate (requirements|security|perf|a11y|i18n|all)
  --strict          Fail on warnings (default: fail only on errors)
  --output=<file>   Save validation report to file

EXAMPLES:
  specify validate --gate=all
  specify validate --gate=security --strict
  specify validate --output=validation-report.md
```

**Output**:
```
üîç Running Spec-Kit Quality Gates...

‚úÖ Gate 1: Requirements Completeness ‚Äî PASS (12/12 checks)
‚úÖ Gate 2: Testability ‚Äî PASS (8/8 checks)
‚ö†Ô∏è Gate 3: Security Review ‚Äî WARNING (2 issues)
   ‚îú‚îÄ STRIDE model incomplete (missing Repudiation)
   ‚îî‚îÄ PII handling not documented
‚ùå Gate 4: Accessibility Compliance ‚Äî FAIL (3 issues)
   ‚îú‚îÄ WCAG level not specified
   ‚îú‚îÄ Touch target sizes not defined
   ‚îî‚îÄ Screen reader testing plan missing
‚úÖ Gate 5: Performance Validation ‚Äî PASS (5/5 checks)
‚ö†Ô∏è Gate 6: Internationalization ‚Äî WARNING (1 issue)
   ‚îî‚îÄ RTL support not specified for multi-locale project

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
üìä Score: 10/12 gates passed (83%)
‚ö†Ô∏è  2 warnings, 1 failure
‚ùå Validation FAILED (use --strict to enforce warnings)
```

---

#### 2. `/speckit.experiment` ‚Äî Experiment Design Generator

```bash
specify experiment [FEATURE] [OPTIONS]

OPTIONS:
  --hypothesis="..."   Hypothesis statement
  --metric=<name>      Primary success metric
  --duration=<days>    Experiment duration (default: 14)

EXAMPLES:
  specify experiment 003-social-proof \
    --hypothesis="Social proof increases signups" \
    --metric=signup_rate \
    --duration=14
```

**Output**: Generates `specs/003-social-proof/experiment.md` with:
- Hypothesis
- Sample size calculation
- Traffic split
- Success criteria
- Guardrail metrics
- Decision framework

---

#### 3. `/speckit.security` ‚Äî Threat Model Generator

```bash
specify security [FEATURE] [OPTIONS]

OPTIONS:
  --model=<type>   Threat model (stride|owasp|custom)
  --sensitivity=<level>  Data sensitivity (low|medium|high|critical)

EXAMPLES:
  specify security 004-payment --model=stride --sensitivity=critical
```

**Output**: Generates security section with:
- STRIDE threat model
- OWASP Top 10 considerations
- Privacy requirements (GDPR/CCPA)
- Compliance checklist

---

#### 4. `/speckit.a11y` ‚Äî Accessibility Requirements Generator

```bash
specify a11y [FEATURE] [OPTIONS]

OPTIONS:
  --level=<wcag>    WCAG level (A|AA|AAA, default: AA)
  --legal=<reqs>    Legal requirements (ada|508|eaa|none)

EXAMPLES:
  specify a11y 005-checkout --level=AA --legal=ada,eaa
```

**Output**: Generates WCAG 2.2 checklist with:
- Success criteria –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ level
- Assistive technology testing plan
- Legal compliance requirements

---

### 6.4 –û–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è /speckit.analyze

**–ù–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ (Passes)**:

**Pass D: Security Validation**
- ‚úÖ All CRITICAL edge cases have security mitigation
- ‚úÖ STRIDE model complete (if data handling feature)
- ‚úÖ PII handling documented (if PII collected)
- ‚ùå FAIL: Missing threat model for authentication feature

**Pass E: Performance Validation**
- ‚úÖ SLOs defined with measurable targets
- ‚úÖ Load test scenarios specified
- ‚úÖ Degradation behavior documented
- ‚ùå FAIL: No performance requirements for API feature

**Pass F: Accessibility Validation**
- ‚úÖ WCAG level specified
- ‚úÖ Touch targets ‚â•44x44px (or justified)
- ‚úÖ Screen reader testing plan exists
- ‚ùå FAIL: No accessibility requirements for UI feature

**Pass G: Internationalization Validation**
- ‚úÖ Supported locales documented
- ‚úÖ RTL support specified (if multi-locale)
- ‚úÖ Text expansion buffer planned
- ‚ö†Ô∏è WARNING: i18n not specified but "global" in description

**Pass H: Analytics Validation**
- ‚úÖ Success metrics defined with targets
- ‚úÖ Event tracking plan exists
- ‚úÖ Privacy compliance verified
- ‚ùå FAIL: No instrumentation plan for measurable feature

---

## 7. ROADMAP –î–õ–Ø –í–ù–ï–î–†–ï–ù–ò–Ø

### Phase 1: Critical Improvements (Q1 2026)
**Goal**: –ü–æ–¥–Ω—è—Ç—å coverage —Å 65% –¥–æ 85%

1. ‚úÖ **–î–æ–±–∞–≤–∏—Ç—å —Å–µ–∫—Ü–∏–∏ –≤ spec-template.md** (1-2 –Ω–µ–¥–µ–ª–∏)
   - Security & Privacy (STRIDE + GDPR)
   - Performance Requirements & SLOs
   - Accessibility Requirements (WCAG 2.2)
   - Analytics & Instrumentation

2. ‚úÖ **–†–∞—Å—à–∏—Ä–∏—Ç—å /speckit.analyze** (1 –Ω–µ–¥–µ–ª—è)
   - Pass D: Security Validation
   - Pass E: Performance Validation
   - Pass F: Accessibility Validation
   - Pass H: Analytics Validation

3. ‚úÖ **–°–æ–∑–¥–∞—Ç—å /speckit.validate** (1-2 –Ω–µ–¥–µ–ª–∏)
   - –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å 12 quality gates
   - CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
   - Markdown report generation

### Phase 2: Enhanced Features (Q2 2026)
**Goal**: AI-augmented workflows

4. ‚úÖ **Context7 Integration –¥–ª—è Phase 0.5** (2 –Ω–µ–¥–µ–ª–∏)
   - API verification —á–µ—Ä–µ–∑ Context7 MCP
   - Dependency documentation fetching
   - Hallucination prevention

5. ‚úÖ **–ù–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã** (2-3 –Ω–µ–¥–µ–ª–∏)
   - `/speckit.security` ‚Äî Threat model generator
   - `/speckit.a11y` ‚Äî Accessibility requirements
   - `/speckit.experiment` ‚Äî Experiment design

6. ‚úÖ **Trade-offs & Alternatives –≤ plan.md** (1 –Ω–µ–¥–µ–ª—è)
   - Design Decisions —Å–µ–∫—Ü–∏—è
   - Assumptions & Risks tracking

### Phase 3: Advanced Capabilities (Q3 2026)
**Goal**: Living documentation & automation

7. ‚úÖ **Auto-generate test stubs** (2 –Ω–µ–¥–µ–ª–∏)
   - `specify test generate` –∏–∑ AS-xxx
   - Framework-specific templates (pytest, jest, vitest)

8. ‚úÖ **Spec-to-code drift detection** (3 –Ω–µ–¥–µ–ª–∏)
   - CI/CD integration
   - LLM-based verification (Semcheck-like)
   - Auto-alert –Ω–∞ drift

9. ‚úÖ **Design tokens import/export** (2 –Ω–µ–¥–µ–ª–∏)
   - Figma plugin integration
   - `specify design import --from=figma://...`
   - Generate CSS/TypeScript from tokens

### Phase 4: Enterprise Features (Q4 2026)
**Goal**: Scale to large organizations

10. ‚úÖ **PRFAQ template** (1 –Ω–µ–¥–µ–ª—è)
    - –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–µ–∫—Ü–∏—è –¥–ª—è product features
    - Amazon-style customer-centric docs

11. ‚úÖ **Internationalization workflow** (2 –Ω–µ–¥–µ–ª–∏)
    - i18n requirements template
    - Pseudo-localization testing
    - Translation service integration

12. ‚úÖ **Living docs deployment** (2 –Ω–µ–¥–µ–ª–∏)
    - `specify docs generate`
    - Auto-deploy to GitHub Pages / Vercel
    - Versioned documentation

---

## 8. BENCHMARK: SPEC-KIT VS WORLD-CLASS STANDARDS

| –ö–∞—Ç–µ–≥–æ—Ä–∏—è | World-Class –°—Ç–∞–Ω–¥–∞—Ä—Ç | Spec-Kit –¢–µ–∫—É—â–∏–π | Gap | –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç |
|-----------|---------------------|------------------|-----|-----------|
| **Requirements Completeness** | SMART requirements, IDs, traceability | ‚úÖ 90% | ‚ö†Ô∏è Weak Non-Goals | Medium |
| **User Stories Depth** | Given-When-Then, edge cases, performance expectations | ‚úÖ 80% | ‚ö†Ô∏è Need quantification | Medium |
| **Edge Case Coverage** | Boundary, input, state, security, concurrency | ‚úÖ 70% | ‚ö†Ô∏è Need discovery framework | High |
| **Security** | STRIDE, OWASP, PII handling, compliance | ‚ùå 20% | üî¥ CRITICAL gap | **CRITICAL** |
| **Performance** | SLOs, load tests, degradation strategy | ‚ö†Ô∏è 40% | üü° Need structure | High |
| **Accessibility** | WCAG 2.2 AA, assistive tech testing | ‚úÖ 60% | ‚ö†Ô∏è Need WCAG 2.2 update | High |
| **Internationalization** | i18n requirements, RTL, locale formatting | ‚ùå 10% | üî¥ Completely missing | Medium |
| **Analytics** | Success metrics, event taxonomy, OpenTelemetry | ‚ö†Ô∏è 30% | üü° Weak instrumentation | High |
| **Design System** | Tokens, component library, HIG integration | ‚úÖ 70% | ‚ö†Ô∏è Token import/export | Low |
| **Trade-offs** | Alternative designs, decision rationale | ‚ùå 15% | üî¥ Google Design Doc missing | **CRITICAL** |
| **Testability** | Test IDs, coverage tracking, automation | ‚úÖ 85% | ‚ö†Ô∏è Auto-generate stubs | Low |
| **API Contracts** | OpenAPI, versioning, deprecation | ‚úÖ 75% | ‚ö†Ô∏è Deprecation workflow | Medium |
| **Living Documentation** | Docs as code, auto-deploy, versioning | ‚ö†Ô∏è 50% | ‚ö†Ô∏è Auto-deploy missing | Low |
| **Experimentation** | Hypothesis, metrics, decision framework | ‚ùå 0% | üî¥ Netflix approach missing | Medium |

**Overall Coverage**: **~65%** of world-class standards

**Top 3 Critical Gaps**:
1. üî¥ **Security & Privacy** (20% coverage) ‚Äî STRIDE, GDPR missing
2. üî¥ **Trade-offs & Alternatives** (15% coverage) ‚Äî Design decisions not documented
3. üü° **Performance SLOs** (40% coverage) ‚Äî Need structured approach

**Quick Wins** (high impact, low effort):
1. Add Security & Privacy section (1 week, +30% security coverage)
2. Add Trade-offs section to plan.md (1 week, +50% design decision coverage)
3. Add Performance SLOs section (1 week, +40% performance coverage)

---

## 9. –ò–°–¢–û–ß–ù–ò–ö–ò

### Specification Frameworks
- [Amazon PRFAQ Guide](https://productstrategy.co/working-backwards-the-amazon-prfaq-for-product-innovation/)
- [Product School PRFAQ Template](https://productschool.com/blog/product-fundamentals/prfaq)
- [Google Design Docs at Google](https://www.industrialempathy.com/posts/design-docs-at-google/)
- [Google Design Doc Template](https://docs.google.com/document/d/1uMHzRsEDZb_p9xfFGerCVhr-0mAi-d-OFY4jJi0dYk4/edit)
- [Pragmatic Engineer: RFCs](https://newsletter.pragmaticengineer.com/p/software-engineering-rfc-and-design)
- [LeadDev: Team Guide to RFCs](https://leaddev.com/software-quality/thorough-team-guide-rfcs)

### Design & Accessibility
- [Apple Human Interface Guidelines](https://developer.apple.com/design/human-interface-guidelines/)
- [iOS Design Guidelines 2025](https://tapptitude.com/blog/i-os-app-design-guidelines-for-2025)
- [WCAG 2.2 ISO Standard](https://www.w3.org/press-releases/2025/wcag22-iso-pas/)
- [WCAG 2025 Compliance Requirements](https://www.accessibility.works/blog/2025-wcag-ada-website-compliance-standards-requirements/)
- [WCAG Complete Guide 2025](https://www.allaccessible.org/blog/wcag-22-complete-guide-2025)

### Testing & Quality
- [Edge Cases in Software Testing](https://muuktest.com/blog/edge-cases-in-software-testing)
- [Error Handling Best Practices](https://www.bomberbot.com/testing/a-beginners-guide-to-testing-error-handling-edge-cases/)
- [Test Coverage Metrics 2025](https://quashbugs.com/blog/test-coverage-metrics-software-testing)
- [Quality Gates Checklist](https://medium.com/@dneprokos/quality-gates-the-watchers-of-software-quality-af19b177e5d1)
- [FDA Software Validation](https://www.fda.gov/media/73141/download)

### Experimentation & Analytics
- [Netflix A/B Testing Platform](http://techblog.netflix.com/2016/04/its-all-about-testing-netflix.html)
- [Netflix Return-Aware Experimentation](https://netflixtechblog.medium.com/return-aware-experimentation-3dd93c94b67a)
- [Netflix Personalization Powerhouse](https://medium.com/@productbrief/netflixs-personalization-powerhouse-how-a-b-testing-at-scale-built-a-300b-streaming-giant-f26804e0d92c)
- [OpenTelemetry Official](https://opentelemetry.io/)
- [Telemetry Best Practices](https://www.splunk.com/en_us/blog/learn/what-is-telemetry.html)

### Internationalization
- [W3C i18n Best Practices](https://www.w3.org/TR/international-specs/)
- [i18n Testing Guide 2025](https://aqua-cloud.io/internationalization-testing/)
- [Software Localization Best Practices](https://www.resolution.de/post/software-localization-best-practices/)

### AI & Emerging Practices
- [Spec-Driven Development 2025](https://www.softwareseni.com/spec-driven-development-in-2025-the-complete-guide-to-using-ai-to-write-production-code/)
- [GitHub Spec-Kit Guide](https://github.blog/ai-and-ml/generative-ai/spec-driven-development-with-ai-get-started-with-a-new-open-source-toolkit/)
- [Specification Drift Prevention](https://www.xugj520.cn/en/archives/ai-code-documentation-sync-tool.html)
- [Software Anti-patterns](https://medium.com/@srinathperera/a-deeper-look-at-software-architecture-anti-patterns-9ace30f59354)

---

## 10. –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï

### –ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã

1. **Spec-Kit —É–∂–µ —Å–∏–ª—å–Ω–µ–µ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∏–Ω–¥—É—Å—Ç—Ä–∏–∏** (65% vs ~40% —Å—Ä–µ–¥–Ω–µ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ)
2. **3 –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö gap'–∞**: Security, Trade-offs, Performance SLOs
3. **Quick wins –¥–æ—Å—Ç—É–ø–Ω—ã**: 3 —Å–µ–∫—Ü–∏–∏ –∑–∞ 3 –Ω–µ–¥–µ–ª–∏ ‚Üí +35% coverage (–¥–æ 85%)
4. **AI-augmented workflows** ‚Äî —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥ —ç–≤–æ–ª—é—Ü–∏–∏
5. **Living documentation** ‚Äî —Ç—Ä–µ–Ω–¥ 2025-2026

### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

**Immediate (1 –º–µ—Å—è—Ü)**:
- ‚úÖ –î–æ–±–∞–≤–∏—Ç—å Security & Privacy —Å–µ–∫—Ü–∏—é
- ‚úÖ –î–æ–±–∞–≤–∏—Ç—å Trade-offs & Alternatives –≤ plan.md
- ‚úÖ –î–æ–±–∞–≤–∏—Ç—å Performance SLOs
- ‚úÖ –†–∞—Å—à–∏—Ä–∏—Ç—å /speckit.analyze (Passes D, E, F)

**Short-term (3 –º–µ—Å—è—Ü–∞)**:
- ‚úÖ –°–æ–∑–¥–∞—Ç—å /speckit.validate —Å 12 quality gates
- ‚úÖ Context7 integration –¥–ª—è API verification
- ‚úÖ –ù–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã: /speckit.security, /speckit.a11y

**Long-term (6-12 –º–µ—Å—è—Ü–µ–≤)**:
- ‚úÖ Auto-generate test stubs
- ‚úÖ Spec-to-code drift detection (CI/CD)
- ‚úÖ Design tokens import/export
- ‚úÖ Living docs deployment

**–¶–µ–ª–µ–≤–æ–π Coverage**: **90%** world-class standards –∫ –∫–æ–Ω—Ü—É 2026 –≥–æ–¥–∞.

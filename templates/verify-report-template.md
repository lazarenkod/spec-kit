# Verification Report Template

This template defines the format for verification reports generated by `/speckit.verify`.

---

```markdown
╔══════════════════════════════════════════════════════════════════╗
║                    Verification Report                            ║
║                    Feature: [feature-name]                         ║
║                    Date: [YYYY-MM-DD HH:MM:SS]                    ║
╠══════════════════════════════════════════════════════════════════╣

## Executive Summary

**Overall Score: [score]% [✅/⚠️/❌] [STATUS]** (Threshold: [threshold]%)

| Category | Pass | Fail | Score | Status |
|----------|------|------|-------|--------|
| Functional (AC) | [pass] | [fail] | [score]% | [status] |
| API Contracts | [pass] | [fail] | [score]% | [status] |
| Visual | [pass] | [fail] | [score]% | [status] |
| Behavior | [pass] | [fail] | [score]% | [status] |
| Performance/NFR | [pass] | [fail] | [score]% | [status] |
| **Overall** | **[total_pass]** | **[total_fail]** | **[overall_score]%** | **[status]** |

**Critical Issues**: [critical_count] blocking failures
**High Priority**: [high_count] issues requiring fixes
**Medium Priority**: [medium_count] optimization opportunities

---

## 1. Acceptance Criteria Verification

**Coverage: [coverage]%** ([pass]/[total] requirements fully met)

| Requirement | AC | Status | Details |
|-------------|-----|--------|---------|
| [REQ-ID] | [AC description] | [✅/❌/⚠️] [Status] | [Details or error message] |

**Failed Tests**:
```
❌ [AS-ID]: [Scenario description]
   Expected: [Expected behavior]
   Actual: [Actual behavior]

   Test: [test-file:line]
   Error: [Error message]
```

---

## 2. API Contract Verification

**Contract Compliance: [compliance]%** ([pass]/[total] endpoints match spec)

| Endpoint | Spec | Actual | Status | Issue |
|----------|------|--------|--------|-------|
| [METHOD /endpoint] | [spec status/schema] | [✅/⚠️] [Match/Drift] | [status] | [issue description] |

**Contract Drift Details**:
```yaml
[METHOD /endpoint]:
  Spec Response:
    - [field]: [type]
  Actual Response:
    - [field]: [type] ([✅/⚠️] [match/drift])
    - [⚠️ Extra field]: [field] (not documented in spec)

  Action: [Update spec.md or remove field from response]
```

---

## 3. Visual Verification

**Visual Regression: [fail_count] failure(s), [warn_count] warning(s)** ([pass_rate]% pass rate)

| Screen | Expected | Actual | Diff | Status |
|--------|----------|--------|------|--------|
| [screen-name] | [baseline] | [screenshot] | [diff]% | [✅/⚠️/❌] [status] |

**[Screen] Failures** ([diff]% visual diff):

1. **[Issue type]: [Element description]**
   - Expected: [Expected state]
   - Actual: [Actual state]
   - Diff: ![diff-image](.verify/diffs/[screen]-[issue].png)

**[Screen] Warnings** ([diff]% visual diff):
- [Minor difference description (acceptable drift)]

---

## 4. Behavior Verification

**Behavior Pass Rate: [pass_rate]%** ([pass]/[total] behaviors correct)

| Behavior | Expected | Actual | Status | Issue |
|----------|----------|--------|--------|-------|
| [behavior-name] | [expected behavior] | [actual behavior] | [✅/❌] [status] | [issue] |

**Failed Behavior**:
```
❌ [Behavior description]
   Expected: [Expected flow]
   Actual: [Actual flow]

   Impact: [Impact description]
   Severity: [CRITICAL/HIGH/MEDIUM]
```

---

## 5. NFR Verification

**Performance: [pass]/[total] passing** ([pass_rate]% pass rate)

| NFR ID | Requirement | Target | Actual | Status | Gap |
|--------|-------------|--------|--------|--------|-----|
| [NFR-ID] | [Description] | [target value] | [actual value] | [✅/❌] [status] | [gap] |

**Lighthouse Report**:
```
Performance: [score]/100 [✅/❌]
  - First Contentful Paint: [time] [✅/❌]
  - Largest Contentful Paint: [time] [✅/❌]
  - Total Blocking Time: [time] [✅/⚠️/❌]
  - Cumulative Layout Shift: [score] [✅/❌]

Accessibility: [score]/100 [✅/⚠️]
Best Practices: [score]/100 [✅/⚠️]
SEO: [score]/100 [✅/⚠️]
```

**Bundle Analysis**:
```
Total: [size] ([over/under] [target] target)
  - [package]: [size] [✅/❌] ([comment])
  - [package]: [size] [✅/❌]
  - Application code: [size] [✅/⚠️]
```

---

## Summary

**Overall Verification Score: [score]%** [✅/⚠️/❌] **[STATUS]**

**Status**: [✅/⚠️/❌] [Above/Below] [threshold]% threshold [- requires fixes]

**Breakdown**:
- ✅ **Strengths**: [List of passing categories/features]
- ❌ **Critical Issues**:
  1. [Critical issue description (BLOCKING)]
  2. [Critical issue description (BLOCKING)]
- ⚠️ **High Priority**:
  1. [High priority issue description]
  2. [High priority issue description]
  3. [High priority issue description]

**Recommended Action**: [Apply auto-fixes for simple issues, manually fix complex behaviors / Continue to next step]

---

## Auto-Fix Suggestions

### Critical (blocking) - Must Fix

**[#]. [Issue description]**
- **File**: `[file-path]:[line]`
- **Current**: `[current code]`
- **Fix**: `[fixed code]`
- **Reasoning**: [Explanation of why this fix is needed]
- **Auto-fix**: [✅ Eligible / ❌ Manual (reason)]

### High Priority - Should Fix

**[#]. [Issue description]**
- **File**: `[file-path]:[line]`
- **Current**: `[current code]`
- **Fix**: `[fixed code]`
- **Impact**: [Impact description, e.g., "Saves ~200KB"]
- **Auto-fix**: [✅ Eligible / ❌ Manual (reason)]

### Medium Priority - Can Defer

**[#]. [Issue description]**
- **File**: `[file-path]`
- **Section**: [Section name if applicable]
- **Action**: [Action description]
- **Auto-fix**: [⚠️ Partial (requires user review)]

---

## Next Steps

**Option 1: Apply Auto-Fixes** (Recommended)
```bash
# Auto-apply [eligible_count] eligible fixes ([fix_numbers])
# Then manually fix critical issue #[number] ([issue])
```

**Option 2: Fix Manually**
```bash
# Review all [total_count] suggestions above
# Apply fixes in order of priority
# Re-run verification after each fix
```

**Option 3: Get Help**
```bash
# Run comprehensive audit for deeper analysis
/speckit.analyze --profile qa
```

---

Apply auto-fixes now? [y/n]: _
╚══════════════════════════════════════════════════════════════════╝
```

## Usage

This template is referenced in the `report-aggregator` agent prompt in `templates/commands/verify.md`:

```yaml
- role: report-aggregator
  prompt: |
    Generate `reports/verify-report.md` using template from `templates/verify-report-template.md`
```

## Variables

The template uses the following placeholder variables that will be replaced by the report-aggregator agent:

### Executive Summary
- `[feature-name]` - Feature name from spec.md
- `[YYYY-MM-DD HH:MM:SS]` - Report generation timestamp
- `[score]` - Overall verification score (0-100)
- `[threshold]` - Pass threshold (default 90)
- `[status]` - PASS/WARN/FAIL
- `[pass]`/`[fail]` - Pass/fail counts per category
- `[critical_count]`/`[high_count]`/`[medium_count]` - Issue counts by severity

### Section-Specific
- `[coverage]` - AC coverage percentage
- `[compliance]` - API contract compliance percentage
- `[pass_rate]` - Test/behavior pass rate percentage
- `[diff]` - Visual diff percentage
- `[REQ-ID]`/`[AS-ID]`/`[NFR-ID]` - Requirement/scenario/NFR identifiers

### Auto-Fix
- `[eligible_count]` - Number of auto-fixable issues
- `[fix_numbers]` - List of fixable issue numbers (e.g., "1, 3, 4, 5")
- `[total_count]` - Total number of suggestions

## Status Emojis

- ✅ PASS - Meets threshold
- ⚠️ WARN - Close to threshold (within 10%)
- ❌ FAIL - Below threshold

## Severity Levels

- **CRITICAL** - Blocking issues (must fix before proceed)
- **HIGH** - Important issues (should fix before merge)
- **MEDIUM** - Optimization opportunities (can defer)
- **LOW** - Minor improvements (optional)

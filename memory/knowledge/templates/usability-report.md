# Usability Test Report Template

> **Purpose**: Document findings from usability testing sessions with actionable recommendations.
> **When to Use**: After conducting usability tests (moderated or unmoderated).
> **Philosophy**: Capture both what happened AND why, leading to specific design improvements.

---

## Report Metadata

| Field | Value |
|-------|-------|
| **Test Name** | [Name of the test/feature tested] |
| **Report Author** | [UX Researcher Name] |
| **Test Date(s)** | [YYYY-MM-DD to YYYY-MM-DD] |
| **Report Date** | [YYYY-MM-DD] |
| **Product/Feature** | [What was tested] |
| **Prototype Fidelity** | [Sketch | Low-fi | High-fi | Production] |

---

## Executive Summary

### One-Paragraph Summary
[2-3 sentences: What was tested, with whom, key findings, and primary recommendation]

### Key Metrics

| Metric | Target | Result | Status |
|--------|--------|--------|--------|
| Task Success Rate | [e.g., 90%] | [Actual %] | âœ…/âš ï¸/âŒ |
| Average Task Time | [e.g., < 2 min] | [Actual time] | âœ…/âš ï¸/âŒ |
| SUS Score | [e.g., > 68] | [Actual score] | âœ…/âš ï¸/âŒ |
| Error Rate | [e.g., < 10%] | [Actual %] | âœ…/âš ï¸/âŒ |

### Top 3 Findings

| # | Finding | Severity | Impact | Recommendation |
|---|---------|----------|--------|----------------|
| 1 | [Critical finding] | Critical | [Who affected] | [Quick fix] |
| 2 | [Major finding] | Major | [Who affected] | [Quick fix] |
| 3 | [Notable finding] | Minor | [Who affected] | [Quick fix] |

---

## 1. Study Overview

### Objectives
1. [Primary objective: What question are we answering?]
2. [Secondary objective]
3. [Secondary objective]

### Research Questions
- RQ1: [Research question 1]
- RQ2: [Research question 2]
- RQ3: [Research question 3]

### Methodology

| Aspect | Details |
|--------|---------|
| **Test Type** | [Moderated Remote | Moderated In-Person | Unmoderated Remote] |
| **Method** | [Think-Aloud | Task-Based | A/B Comparison] |
| **Platform** | [UserTesting | Maze | Zoom | In-Lab] |
| **Duration** | [X minutes per session] |
| **Compensation** | [Amount/Type] |

### Prototype/Version Tested
- **URL/Build**: [Link or build number]
- **Date of Version**: [When this version was created]
- **Known Limitations**: [What doesn't work in this prototype]

---

## 2. Participants

### Recruitment Criteria

| Criterion | Requirement |
|-----------|-------------|
| [Criterion 1] | [e.g., Uses product weekly] |
| [Criterion 2] | [e.g., Manager level+] |
| [Criterion 3] | [e.g., 2+ years in role] |

### Participant Summary

| ID | Role | Experience | Tech Comfort | Completed |
|----|------|------------|--------------|-----------|
| P1 | [Role] | [X years] | [Low/Med/High] | âœ… |
| P2 | [Role] | [X years] | [Low/Med/High] | âœ… |
| P3 | [Role] | [X years] | [Low/Med/High] | âœ… |
| P4 | [Role] | [X years] | [Low/Med/High] | âœ… |
| P5 | [Role] | [X years] | [Low/Med/High] | âœ… |

### Demographics

| Category | Distribution |
|----------|--------------|
| Gender | [X female, Y male, Z non-binary] |
| Age Range | [Range] |
| Location | [Geographic spread] |
| Device | [X desktop, Y mobile, Z tablet] |

**Note on Sample Size**: [Justification for N participants - typically 5 users find ~85% of usability issues]

---

## 3. Tasks Tested

### Task Overview

| # | Task Description | Success Criteria | Target Time |
|---|------------------|------------------|-------------|
| T1 | [Task 1 description] | [What constitutes success] | [X min] |
| T2 | [Task 2 description] | [What constitutes success] | [X min] |
| T3 | [Task 3 description] | [What constitutes success] | [X min] |
| T4 | [Task 4 description] | [What constitutes success] | [X min] |

### Task Wording (Exact)

**Task 1**: "[Exact wording given to participants]"

**Task 2**: "[Exact wording given to participants]"

**Task 3**: "[Exact wording given to participants]"

**Task 4**: "[Exact wording given to participants]"

---

## 4. Results by Task

### Task 1: [Task Name]

**Success Rate**: X/5 (XX%)

| Participant | Success | Time | Path Taken | Notes |
|-------------|---------|------|------------|-------|
| P1 | âœ… | 1:23 | [Optimal] | [Observation] |
| P2 | âš ï¸ (with help) | 3:45 | [Deviated at X] | [Observation] |
| P3 | âŒ | - | [Abandoned at X] | [Observation] |
| P4 | âœ… | 2:01 | [Optimal] | [Observation] |
| P5 | âœ… | 1:45 | [Minor deviation] | [Observation] |

**Key Observations**:
- [Pattern 1 observed across participants]
- [Pattern 2 observed across participants]

**Quote**: "[Direct participant quote that illustrates the experience]" â€” P2

---

### Task 2: [Task Name]

**Success Rate**: X/5 (XX%)

| Participant | Success | Time | Path Taken | Notes |
|-------------|---------|------|------------|-------|
| P1 | | | | |
| P2 | | | | |
| P3 | | | | |
| P4 | | | | |
| P5 | | | | |

**Key Observations**:
- [Pattern observed]
- [Pattern observed]

---

### Task 3: [Task Name]

[Same structure as above]

---

### Task 4: [Task Name]

[Same structure as above]

---

## 5. Detailed Findings

### Severity Scale

| Level | Description | Impact | Action Needed |
|-------|-------------|--------|---------------|
| **Critical** | Prevents task completion | Users cannot achieve goal | Fix before launch |
| **Major** | Significantly impedes task | Causes frustration, errors | Fix in next sprint |
| **Minor** | Causes minor friction | Annoyance, slight delays | Add to backlog |
| **Enhancement** | Opportunity for delight | Not a problem, but could be better | Consider for future |

---

### Finding 1: [Finding Title]

**Severity**: ğŸ”´ Critical | ğŸŸ  Major | ğŸŸ¡ Minor | ğŸŸ¢ Enhancement

**Frequency**: [X/5 participants affected]

**Nielsen Heuristic Violated**: [If applicable]
- [ ] Visibility of system status
- [ ] Match between system and real world
- [ ] User control and freedom
- [ ] Consistency and standards
- [ ] Error prevention
- [ ] Recognition rather than recall
- [ ] Flexibility and efficiency of use
- [ ] Aesthetic and minimalist design
- [ ] Help users recognize, diagnose, recover from errors
- [ ] Help and documentation

**Description**:
[What happened and why it matters]

**Evidence**:
- P1: "[Quote or behavior]"
- P3: "[Quote or behavior]"
- P4: "[Quote or behavior]"

**Screenshot/Recording Reference**:
[Timestamp or screenshot reference showing the issue]

**Recommendation**:
[Specific, actionable recommendation]

**Before/After Sketch** (if applicable):
```
BEFORE:                    AFTER:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [Current]  â”‚    â†’      â”‚  [Proposed] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Finding 2: [Finding Title]

**Severity**: ğŸ”´ Critical | ğŸŸ  Major | ğŸŸ¡ Minor | ğŸŸ¢ Enhancement

**Frequency**: [X/5 participants affected]

[Same structure as Finding 1]

---

### Finding 3: [Finding Title]

[Same structure]

---

## 6. Quantitative Data

### Task Completion Summary

```
Task Success Rate by Task:

T1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 80%
T2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%
T3: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 40%
T4: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ 60%

Overall: 70%
```

### Time on Task

| Task | Min | Max | Median | Target | Status |
|------|-----|-----|--------|--------|--------|
| T1 | 0:45 | 2:30 | 1:23 | < 2:00 | âœ… |
| T2 | 1:00 | 4:15 | 2:45 | < 2:00 | âš ï¸ |
| T3 | - | - | - | < 3:00 | âŒ |
| T4 | 2:00 | 5:30 | 3:45 | < 4:00 | âœ… |

### Error Analysis

| Error Type | Frequency | Task(s) | Cause |
|------------|-----------|---------|-------|
| [Error type 1] | [X times] | [Task #] | [Root cause] |
| [Error type 2] | [X times] | [Task #] | [Root cause] |
| [Error type 3] | [X times] | [Task #] | [Root cause] |

### System Usability Scale (SUS)

| Participant | SUS Score |
|-------------|-----------|
| P1 | [Score] |
| P2 | [Score] |
| P3 | [Score] |
| P4 | [Score] |
| P5 | [Score] |
| **Average** | **[Score]** |

**Interpretation**:
- < 51: Poor usability
- 51-68: OK usability
- 68-80.3: Good usability
- > 80.3: Excellent usability

**Our Score**: [Score] = [Interpretation]

---

## 7. Post-Test Questionnaire Results

### Satisfaction Ratings

| Statement | Avg Rating (1-5) |
|-----------|------------------|
| "The interface was easy to use" | [X.X] |
| "I could accomplish my tasks efficiently" | [X.X] |
| "I would use this regularly" | [X.X] |
| "I would recommend this to others" | [X.X] |

### Open-Ended Feedback Themes

**What did you like most?**
- Theme 1: [Summary] (X mentions)
- Theme 2: [Summary] (X mentions)

**What was most frustrating?**
- Theme 1: [Summary] (X mentions)
- Theme 2: [Summary] (X mentions)

**What would you change?**
- Theme 1: [Summary] (X mentions)
- Theme 2: [Summary] (X mentions)

---

## 8. Recommendations

### Priority Matrix

```
                      HIGH IMPACT
                          â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                     â”‚                     â”‚
    â”‚   Quick Wins        â”‚   Major Projects    â”‚
    â”‚   (Do Now)          â”‚   (Plan Carefully)  â”‚
    â”‚                     â”‚                     â”‚
LOW â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HIGH
EFFORT                    â”‚                     EFFORT
    â”‚                     â”‚                     â”‚
    â”‚   Fill-ins          â”‚   Thankless Tasks   â”‚
    â”‚   (If Time)         â”‚   (Avoid/Defer)     â”‚
    â”‚                     â”‚                     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                      LOW IMPACT
```

### Prioritized Recommendations

#### Do Now (Quick Wins)

| # | Recommendation | Finding | Effort | Impact |
|---|----------------|---------|--------|--------|
| 1 | [Specific action] | F1 | Low | High |
| 2 | [Specific action] | F3 | Low | High |

#### Plan Carefully (Major Projects)

| # | Recommendation | Finding | Effort | Impact |
|---|----------------|---------|--------|--------|
| 3 | [Specific action] | F2 | High | High |

#### Consider Later (Fill-ins)

| # | Recommendation | Finding | Effort | Impact |
|---|----------------|---------|--------|--------|
| 4 | [Specific action] | F4 | Low | Low |

#### Not Recommended (Thankless Tasks)

| # | Recommendation | Why Not | Alternative |
|---|----------------|---------|-------------|
| - | [Potential action] | [High effort, low impact] | [Better option] |

---

## 9. Limitations & Next Steps

### Study Limitations
- [Limitation 1: e.g., Small sample size]
- [Limitation 2: e.g., Prototype fidelity affected X]
- [Limitation 3: e.g., Participants were not first-time users]

### Questions for Further Research
- [Question 1 that emerged from testing]
- [Question 2 that emerged from testing]

### Recommended Next Steps
1. [ ] [Immediate action item]
2. [ ] [Short-term action item]
3. [ ] [Follow-up research to consider]

### Re-Test Plan
- **When**: [After implementing top recommendations]
- **Focus**: [Specific areas to revalidate]
- **Participants**: [Same or different recruitment criteria]

---

## Appendix

### A. Test Script
[Link to or include the test script/protocol]

### B. Participant Screener
[Link to or include the screener questions]

### C. Raw Data
[Link to spreadsheet with all data]

### D. Session Recordings
| Participant | Recording Link | Duration | Highlights |
|-------------|----------------|----------|------------|
| P1 | [Link] | [XX:XX] | [Timestamp of key moment] |
| P2 | [Link] | [XX:XX] | [Timestamp of key moment] |

### E. Prototype/Design Files
[Link to Figma or prototype used]

---

# Usability Report Quality Checklist

## Methodology
- [ ] Objectives clearly stated
- [ ] Research questions defined
- [ ] Participant criteria documented
- [ ] Tasks are realistic and specific
- [ ] Success criteria defined for each task

## Findings
- [ ] Severity ratings applied consistently
- [ ] Nielsen heuristics referenced where applicable
- [ ] Participant quotes included as evidence
- [ ] Patterns across participants identified
- [ ] Quantitative data supports qualitative findings

## Recommendations
- [ ] Each finding has an actionable recommendation
- [ ] Recommendations are specific, not vague
- [ ] Priority matrix guides decision-making
- [ ] Quick wins identified for immediate action

## Reporting
- [ ] Executive summary can stand alone
- [ ] Limitations acknowledged
- [ ] Next steps defined
- [ ] Raw data/recordings accessible
